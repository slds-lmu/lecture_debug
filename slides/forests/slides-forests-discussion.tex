\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}


\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Advantages and Disadvantages
}{% Relative path to title page image: Can be empty but must not start with slides/
}{% Learning goals, wrapped inside itemize environment
  \item Know advantages and disadvantages of random forests
  \item Be able to explain random forests in terms of hypothesis space, risk and optimization
}


\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item All advantages of trees also apply to RF: not much preprocessing required, missing value handling, etc.
  \item Easy to parallelize
  \item Often works well (enough)
  \item Integrated variable importance
  \item Integrated estimation of generalization performance via OOB error
  \item Works well on high-dimensional data 
  \item Works well on data with irrelevant \enquote{noise} variables
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often sub-optimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees (but many extra tools are nowadays
    available for interpreting RFs)
  % \item Less aggressive risk minimization compared to boosting
  \item Implementation can be memory-hungry
  \item Prediction is computationally demanding for large ensembles
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Synopsis}
\textbf{Hypothesis Space:}\\
Random forest models are (sums of) step functions over rectangular partitions of (subspaces of) $\Xspace$.\\
Their maximal complexity is controlled by the number of trees in the random forest ensemble and the stopping criteria for the constituent trees.

\lz

\textbf{Risk:}\\
Like trees, random forests can use any kind of loss function for regression or classification.

\lz

\textbf{Optimization:}\\
Exhaustive search over all (randomly selected!) candidate splits in each node of each tree to minimize the empirical risk in the child nodes.\\ 

{\small
Like all bagging methods, optimization can be done in parallel over the ensemble members.}

\end{vbframe}

\endlecture
\end{document}
