\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\input{../../latex-math/ml-ensembles.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Feature Importance
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/cart_forest_fimp_1a  
}{% Learning goals, wrapped inside itemize environment
  \item Understand that the goal of defining variable importance is to enhance interpretability of the random forest
  \item Know definition of variable importance based on improvement in split criterion
  \item Know definition of variable importance based on permutations of OOB observations
}

\begin{vbframe}{Variable Importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random forests as ensembles of trees lose this feature
  \item Contributions of the different features to the model are difficult to evaluate
  \item Way out: variable importance measures
  \item Basic idea: by how much would the performance of the random forest decrease if a specific feature were removed or rendered useless?
\end{itemize}


\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[0]
    \For{features $x_j$, $j = 1$ to $p$}
    \For{tree base learners $\blh$, $m = 1$ to $M$}
    \State {Find all nodes $\Np$ in $\blh$ that use $x_j$.} 
    \State {Compute improvement in splitting criterion achieved by them.}
    \State {Add up these improvements.}
    \EndFor
    \State {Add up improvements over all trees to get feature importance of $x_j$.}
    %\State {This is the feature importance for $x_j$.}
    \EndFor
  \end{algorithmic}
\end{algorithm}
\vskip -2em
\begin{figure}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}


\end{knitrout}
% \caption{Two importance measures on the iris dataset.}
\end{figure}

\framebreak

\begin{minipage}[b]{0.35\textwidth}
  % FIGURE SOURCE: https://docs.google.com/drawings/d/1A0hdwZrwHI8zwMdchY-rn0bpGcNYO-Y4te20DMSwQ84/edit
  \includegraphics[width = 0.9\textwidth]{figure_man/forests-featimp-new}
\end{minipage}%
\begin{minipage}[b]{0.65\textwidth}
  \begin{algorithm}[H]
  \scriptsize
  \caption*{\small{Measure based on permutations of OOB obs.}}
  \begin{algorithmic}[0]
    \State Estimate OOB error $\widehat{\text{err}}_{\text{OOB}}$.
    \For{features $x_j$, $j = 1$ to $p$}
      \State {Perform permutation $\psi_j$ on $x_j$ to distort \\ 
      \phantom{\textbf{for}} feature-target relation for $x_j$.}
    \For{distorted observations $(\mathbf{x}^{(i)}_{\psi_j}, \yi)$, 
    $i = 1$ to $n$}
      % \State {Find trees for which $\xyi$ is OOB.}
      \State {Compute OOB prediction $\yih_{\text{OOB}, \psi_j}$.}
      \State {Compute corresponding loss 
      $L(\yi, \yih_{\text{OOB}, \psi_j})$.}
    \EndFor
    \State {Estimate importance of $j$-th variable}
    \State {$\widehat{\text{VI}_j} = \widehat{\text{err}}_{\text{OOB}, \psi_j} - 
    \widehat{\text{err}}_{\text{OOB}} $}
    \State {\phantom{$\widehat{\text{VI}_j}$} 
    $= \meanin L(\yi, \yih_{\text{OOB}, \psi_j}) - 
    \widehat{\text{err}}_{\text{OOB}}$.}
    \EndFor
  \end{algorithmic}
\end{algorithm}
\end{minipage}


\framebreak

\begin{itemize}
  \item Measure based on improvement in split criterion: MeanDecreaseGini (average total decrease in node impurities, measured by the Gini index)
  \item Measure based on permutations of OOB observations: MeanDecreaseAccuracy (average decrease in accuracy for predictions of OOB observations after permuting the $j$-th feature)
\end{itemize}
{\includegraphics[width=0.95\textwidth]{figure/cart_forest_fimp_1}} 
% \framebreak

% <<>>=
% lrn = makeLearner("classif.randomForest", importance = TRUE)
% mod = train(lrn, iris.task)
% mlr::getFeatureImportance(mod)
% @

% \framebreak

% <<echo=TRUE, size="footnotesize", fig.height=4>>=
% rf = getLearnerModel(mod)
% randomForest::varImpPlot(rf,
%   main = "Variable Importance")
% @

% \framebreak

% <<echo=TRUE, size="footnotesize", fig.height=4>>=
% v = generateFilterValuesData(iris.task,
%   method = c("rf.importance", "cforest.importance"))
% plotFilterValues(v)
% @


\end{vbframe}


\endlecture
\end{document}
