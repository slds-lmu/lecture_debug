\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Introduction
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/forest.png
}{% Learning goals, wrapped inside itemize environment
  \item Know how random forests are defined by extending the idea of bagging
  \item Understand that the goal is to decorrelate the trees
  \item Understand that the out-of-bag error is a way to obtain unbiased estimates of the generalization error during training
}

\begin{vbframe}{Random Forests}

Modification of bagging for trees proposed by Breiman (2001):

\begin{itemize}
  \item Tree base learners on bootstrap samples of the data
  \item Uses \textbf{decorrelated} trees by randomizing splits (see below)
  \item Tree base learners are usually fully expanded, without aggressive early stopping or
    pruning, to \textbf{increase variance of the ensemble}
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lU1qPlY-NQkvWCDrrV6PdDYkloF3eFudM9v2NFYg1rY/edit?usp=sharing
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}
\end{vbframe}



\begin{vbframe}{Random feature sampling}

\begin{itemize}
  \item From our analysis of bagging risk we can see that decorrelating trees improves the ensemble
  \item Simple randomized approach:\\
    At each node of each tree, randomly draw $\text{mtry} \le p$ candidate features to consider for splitting. Recommended values:
  \begin{itemize}
    \item Classification: mtry $ = \lfloor \sqrt{p} \rfloor$
    \item Regression: mtry $ = \lfloor p/3 \rfloor$
  \end{itemize}
\end{itemize}
\end{vbframe}

% \begin{Random Forest Algorithm}
%   \begin{algorithm}[H]
%   \caption*{Random Forest algorithm}
%   \begin{algorithmic}[1]
%   \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
%   in the forest, number $\texttt{mtry}$ of variables to draw for each split
%   \For {$m = 1 \to M$}
%   \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
%   \State Grow tree $\bl$ using $\D^{[m]}$
%   \State For each split only consider $\texttt{mtry}$ randomly selected features
%   \State Grow tree without early stopping or pruning
% \EndFor
% \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
% \end{algorithmic}
% \end{algorithm}
% \end{vbframe}

\begin{vbframe}{Effect of ensemble size}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1} 

}



\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2} 

}



\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3} 

}



\end{knitrout}
\end{vbframe}

\begin{vbframe}{Out-of-Bag Error Estimate}
With the RF it is possible to obtain unbiased estimates of the generalization error directly
during training, based on the out-of-bag observations for each tree:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_4} 

}



\end{knitrout}

\framebreak

\begin{center}
  % https://docs.google.com/drawings/d/1rxWgtVJEnurCPQ3njgDZ_JWPuQmiMfUttVU7q3Dr3Zs/edit
  \includegraphics[width=0.7\textwidth]{figure_man/forests-oob-error-2}
\end{center}

\footnotesize

\begin{itemize}
  \item For an estimation of the generalization error, we exploit the 
  fact that the $i$-th observation acts as unseen test point for all trees in 
  which it is OOB. 
  \item Let $\text{OOB}^{[m]}$ denote the index set 
  $\left \{i \in \nset | (\xv^{(i)}, \yi) \text{ is OOB for } \bl \right \}$.
  \item The number of trees for which the $i$-th observation is OOB is then 
  given by $S_{\text{OOB}}^{(i)} = 
  \sum_{m = 1}^M \I(i \in \text{OOB}^{[m]})$.
  \item We can compute the average over predictions $\hat{y}^{(i)[m]}$ from 
  trees $\bl$ that have observation $i$ in their OOB data to obtain an 
  ensemble prediction.
  \item The average loss of these ensemble OOB predictions over all $n$ 
  observations yields an estimate for the generalization error.
  \item Compute the ensemble OOB prediction for each observation:
  $$\yih_{\text{OOB}} = \begin{cases}
  \frac{1}{S_{\text{OOB}}^{(i)}} \sum_{m = 1}^{M} 
  \I(i \in \text{OOB}^{[m]}) \cdot \hat{y}^{(i)[m]} & \text{in regression,} 
  \\ \phantom{x} \\
  \argmax_{k \in \gset} 
  \frac{\sum_{m = 1}^{M} \I(i \in \text{OOB}^{[m]}) \cdot 
  \I(\hat{h}^{(i)[m]} = k)} {S_{\text{OOB}}^{(i)}} &
  \text{in classification.}
  \end{cases}$$
  \item Then, take the average of the resulting point-wise losses to estimate 
  the OOB error of the forest:
  $$\widehat{\text{err}}_{\text{OOB}} = \meanin L(\yi, \yih_{\text{OOB}})$$
  \item Note that the use of class labels commands the use of 0-1 loss in 
  classification (alternative formulations for other losses are possible).
  \item OOB size: $\P(i \in \text{OOB}^{[m]}) = \left(1 - \frac{1}{n}\right)^n 
  \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx 0.37$ for 
  $i \in \nset$.
  \item Similar to 3-CV, can be used for a quick model selection.
\end{itemize}


\end{vbframe}

\endlecture
\end{document}
