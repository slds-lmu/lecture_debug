\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Benchmarking Trees, Forests, and Bagging K-NN
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/bm_stable_vs_unstable.pdf
}{% Learning goals, wrapped inside itemize environment
  \item Understand for which kind of learners bagging can improve predictive power
}


\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  \begin{itemize}
    \item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
    \item Algorithms:
    \begin{itemize}
      \item classification tree (CART, implemented in \code{rpart}, \code{max.depth}: 30, \code{min.split}: 20, \code{cp}: 0.01)
      \item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
      \item k-nearest neighbors (k-NN, implemented in \code{kknn}, $k=7$)
      \item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
      \item random forest with 50 trees (implemented in \code{randomForest})
    \end{itemize}
    \item Method to evaluate performance: 10-fold cross-validation
    \item Performance measure: mean misclassification error on test sets
    \end{itemize}

    \framebreak

    \begin{itemize}
    \item Datasets from \pkg{mlbench}:
    \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass (6 levels) on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

\framebreak

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/bm_stable_vs_unstable.pdf}\end{center}{}

%\framebreak

\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  Bagging k-NN does not improve performance because:

  \begin{itemize}
    \item k-NN is stable w.r.t. perturbations
    \item In a 2-class problem, nearest-neighbor-based classification only changes under bagging if both
    \begin{itemize}
    \item the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples, but the probability that any given observation is in the bootstrap sample is 63\%, which is greater than 50\%,
    \item and, simultaneously, the \emph{new} nearest neighbor(s) all have a different label than the missing nearest neighbor in those bootstrap samples, which is unlikely for most regions of $\Xspace \times \Yspace$.
    \end{itemize}
\end{itemize}
\end{vbframe}


\endlecture
\end{document}
